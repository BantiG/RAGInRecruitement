{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19101a61-0586-465b-8a2f-c2207e910e62",
   "metadata": {},
   "source": [
    "Recruitment Industry Use Case: Job Description and Resume Matching leveraging Llama-2-7b, GPT-J Embeddings, FAISS, and Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8c832b",
   "metadata": {},
   "source": [
    "In this notebook, we will deploy the Llama-2-7b model using the Python 3 kernel and the ml.t3.medium instance type. The model will function as a generation model for generating responses.\n",
    "\n",
    "The SageMaker endpoint instance to be used is ml.g5.4xlarge.\n",
    "\n",
    "To perform inference on the Llama models, include custom_attributes='accept_eula=true' in the header to confirm acceptance of the end-user license agreement (EULA). By default, custom_attributes='accept_eula=false' is set, so inference requests will fail unless explicitly changed to true.\n",
    "\n",
    "This notebook involves loading resumes and job descriptions using DirectoryLoader, splitting them into smaller chunks with RecursiveCharacterTextSplitter. GPT-J 6B generates embeddings for both, which are stored in a FAISS vector store for efficient similarity search. When a recruiter queries the system, FAISS retrieves the most relevant job descriptions and resumes. A prompt template is created to pass this context to the Llama-2-7b model, deployed via SageMaker JumpStart, which then generates a response such as a recommendation score or fitment summary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05963628-8823-4fd7-ac81-8a2614cf5020",
   "metadata": {},
   "source": [
    "Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669ab274-38f2-49e9-8691-9ec3e969ea1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install faiss-cpu==1.7.4 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f8181e-d882-4dd3-b928-4d7a261ed965",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain==0.0.222 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cac72b-33e0-4213-88b7-1e0fecd8effc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "!pip install PyYAML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea929e6-3eb8-40b6-a7eb-28c8eebf4040",
   "metadata": {},
   "source": [
    "Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994c9d96-ad0a-45f5-a5e3-1dd386ec9f22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import logging\n",
    "import boto3\n",
    "import yaml\n",
    "import json\n",
    "import numpy as np\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import DirectoryLoader, PDFPlumberLoader\n",
    "from langchain.embeddings.sagemaker_endpoint import EmbeddingsContentHandler\n",
    "from langchain.embeddings import SagemakerEndpointEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "from multiprocessing import cpu_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aef7657-172f-458d-baef-5c7a1f0096f0",
   "metadata": {},
   "source": [
    "Setting up Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca7288d-94ca-4a4d-97dc-cd48fdad4a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('sagemaker')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "print(logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103e03c3-d67e-4c79-a11b-1b285f1e46c5",
   "metadata": {},
   "source": [
    "Log versions of dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f2962c-0540-4d0a-ab64-ca22b1b02395",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f'Using requests=={requests.__version__}')\n",
    "logger.info(f'Using pyyaml=={yaml.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48c98d0-2961-41e4-bfe7-1795b705b2ee",
   "metadata": {},
   "source": [
    "SageMaker endpoint and region setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda7f661-5d38-4d4e-afa7-cf9a46f7ac37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-1\"\n",
    "TEXT_EMBEDDING_MODEL_ENDPOINT_NAME = 'huggingface-textembedding-gpt-j-6b-fp16-1705613925'\n",
    "MODEL_ID = 'huggingface-textembedding-gpt-j-6b-fp16'  \n",
    "MODEL_VERSION = '*'\n",
    "INSTANCE_TYPE = 'ml.g4dn.2xlarge'\n",
    "INSTANCE_COUNT = 1\n",
    "IMAGE_SCOPE = 'inference'\n",
    "MODEL_DATA_DOWNLOAD_TIMEOUT = 3600  # in seconds\n",
    "CONTAINER_STARTUP_HEALTH_CHECK_TIMEOUT = 3600\n",
    "CONTENT_TYPE = 'application/json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146edb84-2021-4b30-86e2-161ce674c574",
   "metadata": {},
   "source": [
    "Set up roles and clients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de24d73-ddaa-45e7-9763-99cc17384e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('sagemaker-runtime')\n",
    "REGION_NAME = boto3.session.Session().region_name\n",
    "print(REGION_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcd9af2-f37d-4e93-9cd1-cce038ce0a9f",
   "metadata": {},
   "source": [
    "Load Resumes and Job Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce8815b-f7e0-454c-8bf2-c07f5e30d46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pdfplumber\n",
    "loader = DirectoryLoader(\"./resumes/\", glob=\"**/*.pdf\", loader_cls=PDFPlumberLoader)\n",
    "job_description_loader = DirectoryLoader(\"./job_descriptions/\", glob=\"**/*.txt\", loader_cls=TextLoader)\n",
    "\n",
    "resumes = loader.load()\n",
    "job_descriptions = job_description_loader.load()\n",
    "\n",
    "print(resumes[0].page_content)\n",
    "print(resumes[0].metadata)\n",
    "\n",
    "print(job_descriptions[0].page_content)\n",
    "print(job_descriptions[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1341a4f3-44fa-4102-acd1-c62a7d4c5743",
   "metadata": {},
   "source": [
    "Split documents into smaller chunks (to handle large texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492ddcda-94ce-42f1-afbf-a1032642a472",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "resume_chunks = text_splitter.split_documents(resumes)\n",
    "job_description_chunks = text_splitter.split_documents(job_descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24d4c4c-d9d8-467f-8ed9-3fb0e42e8717",
   "metadata": {},
   "source": [
    "Print information about document length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b719f1f9-f193-4a6a-8b85-8624184bc5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_doc_length = lambda documents: sum([len(doc.page_content) for doc in documents]) // len(documents)\n",
    "avg_char_count_pre = avg_doc_length(resumes)\n",
    "avg_char_count_post = avg_doc_length(resume_chunks)\n",
    "print(f'Average length among {len(resumes)} resumes loaded is {avg_char_count_pre} characters.')\n",
    "print(f'After the split we have {len(resume_chunks)} resume chunks. Average length is {avg_char_count_post} characters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17e050d-95ee-42c0-a94f-c1a693b35926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for resumes and job descriptions using GPT-J model via SageMaker\n",
    "\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "class ContentHandler(EmbeddingsContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, inputs: list[str], model_kwargs: Dict) -> bytes:\n",
    "        input_str = json.dumps({\"text_inputs\": inputs, **model_kwargs})\n",
    "        return input_str.encode(\"utf-8\")\n",
    "\n",
    "    def transform_output(self, output: bytes) -> List[List[float]]:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[\"embedding\"]\n",
    "\n",
    "content_handler = ContentHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabf5388-5a97-49cb-9798-849a478c7d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SageMaker embedding model for resumes and job descriptions\n",
    "embedding_model = SagemakerEndpointEmbeddings(\n",
    "    endpoint_name=TEXT_EMBEDDING_MODEL_ENDPOINT_NAME,\n",
    "    region_name=REGION_NAME,\n",
    "    content_handler=content_handler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36440f0a-7de4-40c2-a749-5de82f9238fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for resumes\n",
    "resume_embeddings = [embedding_model.embed_query(resume.page_content) for resume in resume_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b93459-fef6-43e0-a6f1-5e01c8488f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for job descriptions\n",
    "job_description_embeddings = [embedding_model.embed_query(job_desc.page_content) for job_desc in job_description_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfe50fd-5fb7-470d-b69a-762d8d776377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FAISS vector stores for efficient retrieval\n",
    "faiss_resumes = FAISS.from_documents(resume_chunks, embedding_model)\n",
    "faiss_job_descriptions = FAISS.from_documents(job_description_chunks, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb60c683-06ea-49a0-aec5-5abed9907fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add embeddings to FAISS stores\n",
    "faiss_resumes.add_embeddings(resume_embeddings)\n",
    "faiss_job_descriptions.add_embeddings(job_description_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8618b4a6-b0ce-4d76-82c8-133560bd28e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save FAISS index locally\n",
    "faiss_resumes.save_local(\"faiss_resume_index\")\n",
    "faiss_job_descriptions.save_local(\"faiss_job_description_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faf3123-86ce-4ace-9d5e-385b38d984bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search and retrieve relevant job descriptions based on a query\n",
    "query = \"Looking for a Software Engineer with experience in Java and microservices\"\n",
    "query_embedding = faiss_job_descriptions.embedding_function(query)\n",
    "relevant_job_desc = faiss_job_descriptions.similarity_search_by_vector(query_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8951cc24-91d1-4750-8e32-b1176da7c59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract context from relevant job descriptions\n",
    "context_job_desc = \"\"\n",
    "for job_desc in relevant_job_desc:\n",
    "    context_job_desc += job_desc.page_content\n",
    "context_job_desc = context_job_desc.replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571a7f8f-511c-44ba-a378-44233314b3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search and retrieve relevant resumes based on the retrieved job descriptions\n",
    "context_resume = \"\"\n",
    "for job_desc in relevant_job_desc:\n",
    "    query = f\"Find resumes matching job description: {job_desc.page_content}\"\n",
    "    query_embedding_resume = faiss_resumes.embedding_function(query)\n",
    "    relevant_resumes = faiss_resumes.similarity_search_by_vector(query_embedding_resume)\n",
    "\n",
    "    for resume in relevant_resumes:\n",
    "        context_resume += resume.page_content\n",
    "\n",
    "context_resume = context_resume.replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6d6862-7c8e-4e63-84e9-411a0ab6745d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt template for Llama-2-7b to generate answers\n",
    "template = \"\"\"\n",
    "    As an experienced and detail-oriented assistant, your objective is to provide clear, accurate, and concise responses based on the information provided. \n",
    "    If the answer is not available or unclear, please indicate that you are unable to provide an answer.\n",
    "\n",
    "    CONTEXT:\n",
    "    {context}\n",
    "\n",
    "    =========\n",
    "    QUESTION: {question} \n",
    "    RESPONSE:\n",
    "\"\"\"       \n",
    "prompt = template.format(context=context_resume, question=\"Is this candidate a good fit for the role?\")\n",
    "print(\"Prompt sent to Llama-2-7b:\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496aa987-adae-4cc1-8f4e-81d6b0859ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the Llama-2-7b model on SageMaker JumpStart\n",
    "role = sagemaker.get_execution_role()\n",
    "my_model = JumpStartModel(model_id=\"meta-textgeneration-llama-2-7b-f\")\n",
    "predictor = my_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g5.4xlarge\",\n",
    "    endpoint_name=\"llama-2-recruitment-model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e4d21e-edac-4f46-a4d0-5fb8a2544dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the payload for the model\n",
    "payload = {\n",
    "    \"inputs\": [\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": prompt},\n",
    "            {\"role\": \"user\", \"content\": query},\n",
    "        ]\n",
    "    ],\n",
    "    \"parameters\": {\"max_new_tokens\": 64, \"top_p\": 0.9, \"temperature\": 0.6, \"return_full_text\": False}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b69d4f-feb0-4ba8-87e6-61a7ddbfd3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the response from Llama-2-7b\n",
    "response = predictor.predict(payload, custom_attributes='accept_eula=true')\n",
    "print(\"Response from Llama-2-7b model:\")\n",
    "print(response[0]['generation']['content'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
